{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2283a7d0",
   "metadata": {},
   "source": [
    "## Data Preprocessing for URL dataset pulled from the Mendeley website. \n",
    "\n",
    "### This data will be used in a final course project in CS 549 Fall 2025, San Diego State University.\n",
    "\n",
    "Author: Jia Gapuz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d26e6c",
   "metadata": {},
   "source": [
    "1. Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575bb887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "csv_path = Path(\"URL dataset.csv\")\n",
    "\n",
    "# Read with fallback encoding\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(csv_path, encoding=\"latin-1\")\n",
    "\n",
    "print(f\"Loaded {len(df):,} rows\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46715f45",
   "metadata": {},
   "source": [
    "2. Drop exact duplicates in the dataset, print the ratio of legitimate to malicious URLS in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e771f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log number of rows before removing duplicates\n",
    "original_rows = len(df)\n",
    "\n",
    "#drop exact duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "#log current number of rows and print removed count\n",
    "new_rows = len(df)\n",
    "removed = original_rows - new_rows\n",
    "print(f\"Removed {removed} duplicate rows ({removed/original_rows:.2%} of original).\")\n",
    "\n",
    "# Count each type and compute ratio\n",
    "counts = df['type'].value_counts(dropna=False)\n",
    "ratio = (counts / counts.sum()).rename('ratio')\n",
    "summary = pd.concat([counts.rename('count'), ratio], axis=1)\n",
    "\n",
    "print(\"\\nCounts and ratios by type:\")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214b5bc8",
   "metadata": {},
   "source": [
    "Since the current dataset's ratio is unaaceptable (we defined acceptable as at least 60-40), we will pull from another dataset to reach a 50-50 if possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d68947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate how many more malicious entries are needed for 50-50\n",
    "legit = df['type'].value_counts().get('legitimate', 0)\n",
    "malicious = df['type'].value_counts().get('malicious', 0)\n",
    "entries_needed = max(0, legit - malicious)\n",
    "print(f\"To achieve a 50-50 ratio, you need to add {entries_needed} more malicious entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95400add",
   "metadata": {},
   "outputs": [],
   "source": [
    "phish_df = pd.read_csv(\"Phishing URLs.csv\")\n",
    "\n",
    "# Ensure we append exactly 'entries_needed' new URLs by excluding existing ones first\n",
    "existing_urls = set(df['url'].astype(str)) if 'url' in df.columns else set()\n",
    "candidates = phish_df[\"url\"].astype(str).dropna().drop_duplicates()\n",
    "unique_new = candidates[~candidates.isin(existing_urls)]\n",
    "\n",
    "to_use = unique_new.head(int(entries_needed))\n",
    "rows_to_append = pd.DataFrame({\n",
    "    'url': to_use.values,\n",
    "    'type': ['phishing'] * len(to_use)\n",
    "})\n",
    "\n",
    "#append entries\n",
    "df = pd.concat([df, rows_to_append], ignore_index=True)\n",
    "\n",
    "#turn all 'malicious' labels into 'phishing'\n",
    "df['type'] = df['type'].replace({'malicious': 'phishing'})\n",
    "\n",
    "#print summary\n",
    "counts = df['type'].value_counts(dropna=False)\n",
    "ratio = (counts / counts.sum()).rename('ratio')\n",
    "summary = pd.concat([counts.rename('count'), ratio], axis=1)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e603b163",
   "metadata": {},
   "source": [
    "3. Ensure all entries are in lowercase, relabel legitimate entries to a 0 and phishing to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9587050",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "for col in obj_cols:\n",
    "    mask = df[col].notna()\n",
    "    df.loc[mask, col] = df.loc[mask, col].str.strip().str.lower()\n",
    "\n",
    "#relabel the type column to 1 or 0\n",
    "if 'type' in df.columns:\n",
    "    df['type'] = df['type'].map({'legitimate': 0, 'phishing': 1})\n",
    "\n",
    "#display a few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1028c9d9",
   "metadata": {},
   "source": [
    "4. Use a parsing library (ie urllib.parse) to extract and append the following information:\n",
    "\n",
    "Scheme\n",
    "Subdomain\n",
    "Registrable domain\n",
    "Suffix\n",
    "Path\n",
    "Query\n",
    "Fragment\n",
    "Port\n",
    "Username\n",
    "Password\n",
    "Host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3ce022",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "# Optional robust domain parsing\n",
    "try:\n",
    "    import tldextract\n",
    "    has_tldextract = True\n",
    "except ImportError:\n",
    "    has_tldextract = False\n",
    "\n",
    "\n",
    "def parse_url(u: str):\n",
    "    try:\n",
    "        parsed = urlparse(u)\n",
    "    except ValueError:\n",
    "        # Handle malformed URLs (e.g., invalid IPv6)\n",
    "        return {\n",
    "            'scheme': '',\n",
    "            'subdomain': '',\n",
    "            'registrable_domain': '',\n",
    "            'suffix': '',\n",
    "            'path': '',\n",
    "            'query': '',\n",
    "            'fragment': '',\n",
    "            'port': '',\n",
    "            'username': '',\n",
    "            'password': '',\n",
    "            'host': '',\n",
    "        }\n",
    "    \n",
    "    host = parsed.hostname or ''\n",
    "    # Handle port safely - urlparse.port may raise ValueError for invalid ports\n",
    "    try:\n",
    "        port = parsed.port if parsed.port is not None else ''\n",
    "    except ValueError:\n",
    "        # If port cannot be parsed, extract it manually or set to empty\n",
    "        port = ''\n",
    "    username = parsed.username or ''\n",
    "    password = parsed.password or ''\n",
    "    scheme = parsed.scheme or ''\n",
    "    path = parsed.path or ''\n",
    "    query = parsed.query or ''\n",
    "    fragment = parsed.fragment or ''\n",
    "\n",
    "    # Domain parts\n",
    "    if has_tldextract and host:\n",
    "        ext = tldextract.extract(host)\n",
    "        subdomain = ext.subdomain or ''\n",
    "        registrable_domain = (ext.domain + '.' + ext.suffix) if ext.domain and ext.suffix else (ext.domain or '')\n",
    "        suffix = ext.suffix or ''\n",
    "    else:\n",
    "        parts = host.split('.') if host else []\n",
    "        suffix = parts[-1] if len(parts) >= 1 else ''\n",
    "        domain = parts[-2] if len(parts) >= 2 else ''\n",
    "        subdomain = '.'.join(parts[:-2]) if len(parts) >= 3 else ''\n",
    "        registrable_domain = (domain + ('.' + suffix if suffix else '')) if domain else ''\n",
    "\n",
    "    return {\n",
    "        'scheme': scheme,\n",
    "        'subdomain': subdomain,\n",
    "        'registrable_domain': registrable_domain,\n",
    "        'suffix': suffix,\n",
    "        'path': path,\n",
    "        'query': query,\n",
    "        'fragment': fragment,\n",
    "        'port': port,\n",
    "        'username': username,\n",
    "        'password': password,\n",
    "        'host': host,\n",
    "    }\n",
    "\n",
    "components = df['url'].astype(str).fillna('').apply(parse_url)\n",
    "comp_df = pd.DataFrame(list(components))\n",
    "\n",
    "# Append columns to df (align by index)\n",
    "df = pd.concat([df, comp_df], axis=1)\n",
    "\n",
    "# Preview new columns\n",
    "df[['scheme','subdomain','registrable_domain','suffix','path','query','fragment','port','username','password','host']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d52217",
   "metadata": {},
   "source": [
    "5. Add two new columns to flag http and https entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a410d97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_series = df['url'].astype(str).fillna('')\n",
    "df['is_http'] = np.where(url_series.str.startswith('http://'), 1, 0)\n",
    "df['is_https'] = np.where(url_series.str.startswith('https://'), 1, 0)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee37973c",
   "metadata": {},
   "source": [
    "6. Count and append the following lengths:  \n",
    "    a. Total length  \n",
    "    b. Host length  \n",
    "    c. Path length  \n",
    "    d. Query length  \n",
    "    e. Fragment length  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4237a7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and append URL length features\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Ensure 'url' exists\n",
    "if 'url' not in df.columns:\n",
    "    raise KeyError(\"Column 'url' not found; cannot compute length features.\")\n",
    "\n",
    "url_s = df['url'].astype(str).fillna('')\n",
    "\n",
    "# Prefer already-parsed columns if present; otherwise parse on the fly\n",
    "if 'host' in df.columns:\n",
    "    host_s = df['host'].astype(str).fillna('')\n",
    "else:\n",
    "    host_s = url_s.apply(lambda u: urlparse(u).hostname or '')\n",
    "\n",
    "if 'path' in df.columns:\n",
    "    path_s = df['path'].astype(str).fillna('')\n",
    "else:\n",
    "    path_s = url_s.apply(lambda u: urlparse(u).path or '')\n",
    "\n",
    "if 'query' in df.columns:\n",
    "    query_s = df['query'].astype(str).fillna('')\n",
    "else:\n",
    "    query_s = url_s.apply(lambda u: urlparse(u).query or '')\n",
    "\n",
    "if 'fragment' in df.columns:\n",
    "    fragment_s = df['fragment'].astype(str).fillna('')\n",
    "else:\n",
    "    fragment_s = url_s.apply(lambda u: urlparse(u).fragment or '')\n",
    "\n",
    "# Add length columns\n",
    "df['len_total'] = url_s.str.len()\n",
    "df['len_host'] = host_s.str.len()\n",
    "df['len_path'] = path_s.str.len()\n",
    "df['len_query'] = query_s.str.len()\n",
    "df['len_fragment'] = fragment_s.str.len()\n",
    "\n",
    "print(\"Added length columns: len_total, len_host, len_path, len_query, len_fragment\")\n",
    "\n",
    "df[['len_total','len_host','len_path','len_query','len_fragment']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2266663d",
   "metadata": {},
   "source": [
    "7. Count and append the counts the following characters:  \n",
    "    a. Dots  \n",
    "    b. Slashes  \n",
    "    c. Other special characters (ie -, _, %, @, ?, =, &)  \n",
    "    d. Digits  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0ed618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count and append character occurrence features\n",
    "import pandas as pd\n",
    "\n",
    "if 'url' not in df.columns:\n",
    "    raise KeyError(\"Column 'url' not found; cannot compute character counts.\")\n",
    "\n",
    "url_s = df['url'].astype(str).fillna('')\n",
    "\n",
    "# Core counts\n",
    "df['count_dots'] = url_s.str.count(r'\\.')\n",
    "df['count_slashes'] = url_s.str.count('/')\n",
    "df['count_digits'] = url_s.str.count(r'\\d')\n",
    "\n",
    "# Individual special characters\n",
    "df['count_hyphen'] = url_s.str.count('-')\n",
    "df['count_underscore'] = url_s.str.count('_')\n",
    "df['count_percent'] = url_s.str.count('%')\n",
    "df['count_at'] = url_s.str.count('@')\n",
    "df['count_question'] = url_s.str.count(r'\\?')\n",
    "df['count_equal'] = url_s.str.count('=')\n",
    "df['count_ampersand'] = url_s.str.count('&')\n",
    "\n",
    "# Aggregate special count (from the listed characters)\n",
    "special_cols = [\n",
    "    'count_hyphen','count_underscore','count_percent',\n",
    "    'count_at','count_question','count_equal','count_ampersand'\n",
    "]\n",
    "df['count_special'] = df[special_cols].sum(axis=1)\n",
    "\n",
    "# Preview of new columns\n",
    "df[['count_dots','count_slashes','count_digits','count_special'] + special_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa52bc09",
   "metadata": {},
   "source": [
    "8. Calculate and append the Shannon entropy of the entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3f68e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and append Shannon entropy of URL entries\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "if 'url' not in df.columns:\n",
    "    raise KeyError(\"Column 'url' not found; cannot compute entropy.\")\n",
    "\n",
    "url_s = df['url'].astype(str).fillna('')\n",
    "\n",
    "def shannon_entropy(s: str) -> float:\n",
    "    if not s:\n",
    "        return 0.0\n",
    "    counts = Counter(s)\n",
    "    n = len(s)\n",
    "    return -sum((c/n) * math.log2(c/n) for c in counts.values() if c)\n",
    "\n",
    "# Entropy of full URL string\n",
    "df['entropy_url'] = url_s.apply(shannon_entropy)\n",
    "\n",
    "print(\"Added entropy column: entropy_url\")\n",
    "\n",
    "df[['entropy_url']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb398560",
   "metadata": {},
   "source": [
    "9. Flag the following keywords  \n",
    "a. Login  \n",
    "b. Verify  \n",
    "c. Update  \n",
    "d. Secure  \n",
    "e. Account  \n",
    "f. Bank  \n",
    "g. Paypal  \n",
    "h. Free  \n",
    "i. Prize  \n",
    "j. Gift  \n",
    "k. Confirm  \n",
    "l. Win  \n",
    "m. Signin  \n",
    "n. Support   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ba5ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag URLs containing any of the specified keywords\n",
    "import re\n",
    "\n",
    "if 'url' not in df.columns:\n",
    "    raise KeyError(\"Column 'url' not found; cannot flag keywords.\")\n",
    "\n",
    "keywords = [\n",
    "    'login','verify','update','secure','account','bank','paypal',\n",
    "    'free','prize','gift','confirm','win','signin','support'\n",
    "]\n",
    "pattern = re.compile(r'(' + '|'.join(map(re.escape, keywords)) + r')', flags=re.IGNORECASE)\n",
    "\n",
    "text = df['url'].astype(str).fillna('')\n",
    "df['keyword_flag'] = text.str.contains(pattern, na=False).astype(int)\n",
    "\n",
    "# Ensure the new column appears at the end explicitly\n",
    "cols = list(df.columns)\n",
    "cols.append(cols.pop(cols.index('keyword_flag')))\n",
    "df = df[cols]\n",
    "\n",
    "print(f\"Total rows flagged: {df['keyword_flag'].sum():,}\")\n",
    "\n",
    "df[['url','keyword_flag']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7696f7c",
   "metadata": {},
   "source": [
    "10. Flag link shorteners (ie. bit.ly, t.co, tinyurl.com, goo.gl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d976f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag common link shorteners in URLs\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Known URL shortener registrable domains (expand as needed)\n",
    "shorteners = {\n",
    "    'bit.ly', 't.co', 'tinyurl.com', 'goo.gl'\n",
    "}\n",
    "\n",
    "if 'url' not in df.columns:\n",
    "    raise KeyError(\"Column 'url' not found; cannot flag link shorteners.\")\n",
    "\n",
    "# Choose the best available field for matching\n",
    "if 'registrable_domain' in df.columns:\n",
    "    base = df['registrable_domain'].astype(str).str.lower()\n",
    "elif 'host' in df.columns:\n",
    "    base = df['host'].astype(str).str.lower()\n",
    "else:\n",
    "    base = df['url'].astype(str).fillna('').apply(lambda u: (urlparse(u).hostname or '').lower())\n",
    "\n",
    "# Create flag column\n",
    "is_short = base.isin(shorteners).astype(int)\n",
    "df['is_shortened'] = is_short\n",
    "\n",
    "# Move the new column to the end explicitly\n",
    "cols = list(df.columns)\n",
    "cols.append(cols.pop(cols.index('is_shortened')))\n",
    "df = df[cols]\n",
    "\n",
    "print(f\"Total shorteners flagged: {int(is_short.sum()):,}\")\n",
    "\n",
    "# Preview\n",
    "preview_cols = ['url']\n",
    "if 'registrable_domain' in df.columns:\n",
    "    preview_cols.append('registrable_domain')\n",
    "elif 'host' in df.columns:\n",
    "    preview_cols.append('host')\n",
    "preview_cols.append('is_shortened')\n",
    "df[preview_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a3b6bf",
   "metadata": {},
   "source": [
    "11. Flag all domains that are purely IP numbers (ie http://101.10.1.101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfb93c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag hosts that are pure IP addresses (IPv4 or IPv6)\n",
    "from urllib.parse import urlparse\n",
    "import ipaddress\n",
    "\n",
    "if 'url' not in df.columns:\n",
    "    raise KeyError(\"Column 'url' not found; cannot flag IP hosts.\")\n",
    "\n",
    "# Prefer previously parsed host if available\n",
    "if 'host' in df.columns:\n",
    "    host_s = df['host'].astype(str).fillna('')\n",
    "else:\n",
    "    host_s = df['url'].astype(str).fillna('').apply(lambda u: (urlparse(u).hostname or ''))\n",
    "\n",
    "\n",
    "def is_ip_host(h: str) -> int:\n",
    "    try:\n",
    "        ipaddress.ip_address(h)\n",
    "        return 1\n",
    "    except ValueError:\n",
    "        return 0\n",
    "\n",
    "# Create flag\n",
    "df['is_ip_host'] = host_s.apply(is_ip_host).astype(int)\n",
    "\n",
    "# Move the column to the end explicitly\n",
    "cols = list(df.columns)\n",
    "cols.append(cols.pop(cols.index('is_ip_host')))\n",
    "df = df[cols]\n",
    "\n",
    "print(f\"Total IP hosts flagged: {int(df['is_ip_host'].sum()):,}\")\n",
    "\n",
    "# Preview\n",
    "preview_cols = ['url']\n",
    "if 'host' in df.columns:\n",
    "    preview_cols.append('host')\n",
    "preview_cols.append('is_ip_host')\n",
    "df[preview_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ba016a",
   "metadata": {},
   "source": [
    "12. Save the anotated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c38317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed DataFrame to CSV\n",
    "from pathlib import Path\n",
    "\n",
    "#print out df preview\n",
    "df.head()\n",
    "\n",
    "output_path = Path(\"../processed_urls.csv\")\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Saved {len(df):,} rows x {df.shape[1]} columns to {output_path.resolve()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
